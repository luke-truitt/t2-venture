# -*- coding: utf-8 -*-
"""Paging_Scrape_48209_FULL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UC0y2X8ypR5LtPM-aNrFBEcWwbuCuBZJ
"""

import pandas as pd
from datetime import datetime, timedelta
import requests
import json
import logging
import os
import matplotlib.pyplot as plt

logger = logging.getLogger(__name__)

url = 'https://graphql.cherre.com/graphql'
# Customize these variables.
file_dir = ''  # Must include trailing slash. If left blank, 
# csv will be created in the current directory.
api_email='lukeowentruitt@gmail.com'
api_token ='Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJHcmFwaFFMIFRva2VuIiwibmFtZSI6IiIsImh0dHBzOi8vaGFzdXJhLmlvL2p3dC9jbGFpbXMiOnsieC1oYXN1cmEtYWxsb3dlZC1yb2xlcyI6WyJ0Ml9kZXZlbG9wbWVudCJdLCJ4LWhhc3VyYS1kZWZhdWx0LXJvbGUiOiJ0Ml9kZXZlbG9wbWVudCIsIngtaGFzdXJhLXVzZXItaWQiOiJ0Ml9kZXZlbG9wbWVudCIsIngtaGFzdXJhLW9yZy1pZCI6InQyX2RldmVsb3BtZW50In19.sjHOw5oF3vYb3S_dxhWT7ucJ1qvQccDaHbyjzLkrKQQ'
api_account='Luke Truitt'

def get_graphql_request (Query):
    headers = {'content-type': 'application/json', 'X-Auth-Email': api_email, 'Authorization': api_token}
    # This variable replacement requires Python3.6 or higher
    payload = {"query": Query}
    r = requests.request("POST",url, json=payload, headers=headers)
    return r

def get_graphql_request_variables (Query,Variables):
    headers = {'content-type': 'application/json', 'X-Auth-Email': api_email, 'Authorization': api_token}
    # This variable replacement requires Python3.6 or higher
    payload = {"query": Query, "variables": Variables}
    r = requests.request("POST",url, json=payload, headers=headers)
    return r

def serialize__to_json(cherre, obj):
    """
    Function converts cherre API response to reduced dictionary"""
    if not (isinstance(cherre, requests.models.Response)):
        raise TypeError(
            f"The cherre must be a requests.models.Response, found {type(cherre)}."
        )
    elif not (isinstance(obj, str)):
        raise TypeError(f"The object you are querying must be a str, found {type(obj)}.")
    else:
        if cherre.status_code == 200:
            json_response = json.loads(cherre.content)
            try:
                hits = json_response.get("data").get(obj)
            except AttributeError:
                logger.info(f"No hits found under multimatch for this object query for {obj}.")
                hits = 0
            return hits

def make_query(Query, obj):
    raw_data=get_graphql_request(Query)
    ans=serialize__to_json(raw_data, obj)
    return ans

def make_query_variables(Query, Variables, obj):
    raw_data=get_graphql_request_variables(Query, Variables)
    ans=serialize__to_json(raw_data, obj)
    return ans

data_diff_addr=[]

def tax_avm_desc_page():
  Query_Input='''query MyQuery($previous_id: numeric!) {
  usa_avm(where: {tax_assessor__tax_assessor_id: {_and: {fips_code: {_eq: "48209"}, city: {_eq: "AUSTIN"},
  tax_assessor_id: {_lt: $previous_id}}}},
  distinct_on: tax_assessor_id,
  order_by: {tax_assessor_id: desc},
  limit: 100)
  {
    tax_assessor__tax_assessor_id {
      address
      fips_code
      gross_sq_ft
      city
    }
    tax_assessor_id
    }
  }'''
  return Query_Input

def tax_avm_asc_page():
  Query_Input='''query MyQuery($previous_id: numeric!) {
  usa_avm(where: {tax_assessor__tax_assessor_id: {_and: {fips_code: {_eq: "48209"}, city: {_eq: "AUSTIN"},
  tax_assessor_id: {_gt: $previous_id}}}},
  distinct_on: tax_assessor_id,
  order_by: {tax_assessor_id: asc},
  limit: 100)
  {
    tax_assessor__tax_assessor_id {
      address
      fips_code
      gross_sq_ft
      city
    }
    tax_assessor_id
    }
  }'''
  return Query_Input

#Non-Empty Initialization
last_id=1544919
while (1):
  QI=tax_avm_asc_page()
  VI={"previous_id":last_id}
  OI="usa_avm"
  data_diff=make_query_variables(QI, VI, OI)
  if (not data_diff):
    break
  data_diff_addr=data_diff_addr+data_diff
  last_id=data_diff[len(data_diff)-1]['tax_assessor_id']

from google.colab import files
raw_diff_addr=json.dumps(data_diff_addr)
with open('List_of_48209_Houses.txt', 'w') as f:
  f.write(raw_diff_addr)

files.download('List_of_48209_Houses.txt')

f = open("List_of_48209_Houses.txt", "r")
data=f.read()
diff_addr_48209=json.loads(data)
f.close()

def time_series_query(address_input):
    QI='''query MyQuery {
    usa_avm(where: {tax_assessor__tax_assessor_id: {_and: {city: {_eq: "AUSTIN"}, address: {_eq: "'''+address_input+'''"}}}}) {
    estimated_max_value_amount
    estimated_min_value_amount
    estimated_value_amount
    valuation_date
    tax_assessor__tax_assessor_id {
      address
      fips_code
      gross_sq_ft
      city
    }
    tax_assessor_id
    }
    }'''
    return QI

Obj_Input="usa_avm"

dict_of_48209_ind=dict()
for i in range(len(diff_addr_48209)):
    ad_input=diff_addr_48209[i]['tax_assessor__tax_assessor_id']['address']
    js_output=make_query(time_series_query(ad_input), Obj_Input)
    dict_of_48209_ind[ad_input]=js_output
    dict_of_48209_ind[ad_input]=sorted(dict_of_48209_ind[ad_input], key=lambda x:x ['valuation_date'])

raw_48209_ind=json.dumps(dict_of_48209_ind)
with open('Individual_HouseData_48209.txt', 'w') as f:
  f.write(raw_48209_ind)
files.download('Individual_HouseData_48209.txt')

f = open('Individual_HouseData_48209.txt', "r")
rawdict_of_timeseries=f.read()
dict_of_timeseries=json.loads(rawdict_of_timeseries)
f.close()

list_keys=list(dict_of_timeseries.keys())

street_keys=[]
for i in list_keys:
    starray=i.split(' ')
    starray=tuple(starray[1:])
    ans=' '.join(starray)
    street_keys.append(ans)

from collections import Counter
street_set=set(street_keys)
street_counter=Counter(street_keys)

def street_fn(string):
    starray=string.split(' ')
    starray=tuple(starray[1:])
    ans=' '.join(starray)
    return ans

print (len(street_set))

master_keys=dict()
for i in street_set:
    master_keys[i]=[]
for i in list_keys:
    ans=street_fn(i)
    master_keys[ans].append(i)

master_key_dod=dict()
for i in street_set:
    master_key_dod[i]=dict()
    for j in master_keys[i]:
        master_key_dod[i][j]=dict_of_timeseries[j]
        master_key_dod[i][j]=sorted(master_key_dod[i][j], key=lambda x:x['valuation_date'])

raw_street_data=json.dumps(master_key_dod)
with open('Street_HouseData_48209.txt', 'w') as f:
  f.write(raw_street_data)
files.download('Street_HouseData_48209.txt')

